{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "from help_function import LoadData\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "applist = pd.read_csv('features/applist_tfidf.csv')\n",
    "labelcnt = pd.read_csv('features/label_tfidf.csv')\n",
    "brand = pd.read_csv('features/brand100.csv')\n",
    "h1 = pd.read_csv('features/h1.csv')\n",
    "h2 = pd.read_csv('features/h2_tfidf300.csv')\n",
    "h3 = pd.read_csv('features/h3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trian data , test data\n",
    "# train test data\n",
    "train_datapath =  '../Demo/deviceid_train.tsv' \n",
    "test_datapath =  '../Demo/deviceid_test.tsv' \n",
    "train_data, test_data = LoadData(train_datapath, test_datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = applist.merge(labelcnt, on='device_id', how='left')\n",
    "data = data.merge(brand, on='device_id', how='left')\n",
    "data = data.merge(h1, on='device_id', how='left')\n",
    "data = data.merge(h2, on='device_id', how='left')\n",
    "data = data.merge(h3, on='device_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "# Feature select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FeatureSelect(train_data, label='label', num_class=22, obj='multiclass',\n",
    "                  metric='multi_logloss'):\n",
    "    # binary   , binary_logloss\n",
    "    model = lgb.LGBMClassifier(boosting_type='gbdt',n_estimators=1000, colsample_bytree=1,\n",
    "                               objective = obj,max_depth=3,learning_rate = 0.1,\n",
    "                               num_leaves =31, num_class=num_class,reg_lambda = 1.,\n",
    "                               reg_alpha = 1, n_jobs = -1,random_state = 8082)\n",
    "    # split train valid data\n",
    "    y = train_data[[label]]\n",
    "    data = train_data.drop(['device_id','sex','age','label'],axis=1)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(data, y, test_size=0.1,random_state=666)\n",
    "    # fit\n",
    "    model.fit(x_train, y_train, eval_metric = metric,\n",
    "              eval_set = [(x_train, y_train),(x_valid, y_valid)],\n",
    "              eval_names = ['train','valid'],\n",
    "              early_stopping_rounds = 10, verbose = 0) \n",
    "    feature_importance = pd.DataFrame()\n",
    "    feature_importance['feature'] = x_train.columns.values\n",
    "    feature_importance['importrance'] = model.feature_importances_\n",
    "    useless_feature = feature_importance[feature_importance.importrance==0].feature.tolist()\n",
    "    return useless_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(train_data, test_data,label, num_class, n_folds = 10,\n",
    "         obj='multiclass',metric='multi_logloss'):\n",
    "    \n",
    "    #binary ; log_loss\n",
    "    labels = train_data[[label]]\n",
    "    train_data = train_data.drop(['device_id','sex','age','label'],axis=1)\n",
    "    test_data = test_data.drop(['device_id'],axis=1)\n",
    "    # 10 folds cross validation\n",
    "    KF = KFold(n_splits = n_folds, shuffle = True, random_state = 2018)\n",
    "    # test predictions\n",
    "    nclass = num_class\n",
    "    if num_class == 1:\n",
    "        nclass = 2\n",
    "    test_predictions = np.zeros((test_data.shape[0],nclass))\n",
    "    # validation predictions\n",
    "    out_of_fold = np.zeros((train_data.shape[0],nclass))\n",
    "    # record scores : logloss\n",
    "    train_logloss = []\n",
    "    valid_logloss = []\n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in KF.split(train_data):\n",
    "        # Training data for the fold\n",
    "        train_features = train_data.loc[train_indices, :]\n",
    "        train_labels = labels.loc[train_indices, :]\n",
    "        # Validation data for the fold\n",
    "        valid_features = train_data.loc[valid_indices, :]\n",
    "        valid_labels = labels.loc[valid_indices, :]\n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(boosting_type='gbdt',n_estimators=1000, \n",
    "                                   objective = obj ,max_depth=3,\n",
    "                                   learning_rate = 0.1,  num_leaves =31,num_class=num_class,\n",
    "                                   reg_lambda = 1.,reg_alpha = 1,\n",
    "                                   subsample = 1., n_jobs = -1, random_state = 8082)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = metric,\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], #categorical_feature =['brand','type','btype'],\n",
    "                  early_stopping_rounds = 10, verbose = 0)\n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        # test result\n",
    "        test_predictions+= model.predict_proba(test_data, num_iteration = best_iteration)/n_folds\n",
    "        # valid result\n",
    "        pred_valid = model.predict_proba(valid_features, num_iteration = best_iteration)\n",
    "        # Record the best multi logloss\n",
    "        valid_score = model.best_score_['valid'][metric]\n",
    "        train_score = model.best_score_['train'][metric]\n",
    "        valid_logloss.append(valid_score)\n",
    "        train_logloss.append(train_score)\n",
    "        # validation set result\n",
    "        out_of_fold[valid_indices] = pred_valid\n",
    "        #print('train loss is : %.5f  |  valid loss is : %.5f'%(train_score,valid_score))\n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "    # overall valida\n",
    "    valid_logloss.append(np.mean(valid_logloss))\n",
    "    train_logloss.append(np.mean(train_logloss))\n",
    "    # dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train_logloss':train_logloss,\n",
    "                            'valid_logloss':valid_logloss})\n",
    "    return metrics,out_of_fold,test_predictions\n",
    "\n",
    "def TrainCode(data, label, num_class, nfolds=10,obj='multiclass',\n",
    "              metric='multi_logloss'):\n",
    "    train = train_data.merge(data, on='device_id',how='left')\n",
    "    test = test_data.merge(data, on='device_id', how='left')\n",
    "    useless_feature = FeatureSelect(train, label, num_class, obj, metric)\n",
    "    train.drop(useless_feature, axis=1, inplace=True)\n",
    "    test.drop(useless_feature, axis=1, inplace=True)\n",
    "    metric, train_prob,test_prob = model(train, test,label,num_class,nfolds,obj,metric)\n",
    "    return metric, train_prob, test_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric , lgbtfidf_train , lgbtfidf_test = TrainCode(data, 'label', 22)\n",
    "np.save('new_feature/lgbtfidf_train.npy',lgbtfidf_train)\n",
    "np.save('new_feature/lgbtfidf_test.npy',lgbtfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric, lgbbsex_train, lgbbsex_test = TrainCode(data, 'sex', 1, 10, 'binary','binary_logloss')\n",
    "np.save('new_feature/lgbbsex_train.npy',lgbbsex_train)\n",
    "np.save('new_feature/lgbbsex_test.npy',lgbbsex_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric, lgbbage_train, lgbbage_test = TrainCode(data, 'age', 11, 10)\n",
    "np.save('new_feature/lgbbage_train.npy',lgbbage_train)\n",
    "np.save('new_feature/lgbbage_test.npy',lgbbage_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
